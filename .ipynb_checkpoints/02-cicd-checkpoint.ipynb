{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f2e51e53",
   "metadata": {},
   "source": [
    "Copyright 2023 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0817cd-9e64-4d5e-9c66-4e0961aa1085",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLOps End to End Workflow (II)\n",
    "\n",
    "\n",
    "## CICD:\n",
    "\n",
    "3. Deployment of the Vertex AI Pipeline through a CI/CD process\n",
    "4. Deployment of a Continuous Training pipeline that can be triggered via Pub/Sub and produces a model in the Model Registry\n",
    "5. Deployment of the Inference Pipeline consisting of a Cloud Function that retrieves features from Feature Store and calls the model on a Vertex AI Endpoint\n",
    "6. Deployment of the model to a Vertex AI Endpoint through a CI/CD process.\n",
    "\n",
    "## Prediction:\n",
    "\n",
    "7. Deploy the model to an endpoint\n",
    "8. Create a test prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0f8ad-ca30-4f5c-a135-809409f58abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b997d6a-f8fd-43f5-bafd-81b169965160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3bdeae6-b5c9-479d-8318-627959bf9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mainconfig.yaml') as f:\n",
    "    main_config = yaml.safe_load(f)\n",
    "\n",
    "# select your config    \n",
    "main_config = main_config['creditcards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b2ad96-7e3a-4980-a8d2-ee78993f0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = main_config['project'] \n",
    "REGION = main_config['region'] \n",
    "DOCKER_REPO = main_config['docker_repo']\n",
    "\n",
    "SERVICE_ACCOUNT = main_config['service_account']\n",
    "\n",
    "# BigQuery and data locations\n",
    "\n",
    "BQ_SOURCE_TABLE= main_config['bq']['source_table'] # raw input\n",
    "ML_TABLE = main_config['bq']['ml_table'] # the one we will use for the training\n",
    "\n",
    "BQ_DATASET_NAME = main_config['bq']['dataset']\n",
    "BQ_LOCATION = main_config['bq']['location'] # multiregion provides more resilience\n",
    "\n",
    "VERTEX_DATASET_NAME = main_config['vertex_dataset_name']\n",
    "\n",
    "RAW_SCHEMA_DIR = main_config['raw_schema_dir']\n",
    "\n",
    "BUCKET =  main_config['bucket']\n",
    "\n",
    "# TFX and model config\n",
    "\n",
    "# model version\n",
    "VERSION = main_config['version']\n",
    "\n",
    "\n",
    "MODEL_DISPLAY_NAME = f'{VERTEX_DATASET_NAME}-classifier-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{VERTEX_DATASET_NAME}'\n",
    "\n",
    "MLMD_SQLLITE = 'mlmd.sqllite'\n",
    "ARTIFACT_STORE = os.path.join(WORKSPACE, 'tfx_artifacts_interactive')\n",
    "MODEL_REGISTRY = os.path.join(WORKSPACE, 'model_registry')\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "PIPELINE_ROOT = os.path.join(ARTIFACT_STORE, PIPELINE_NAME)\n",
    "\n",
    "ENDPOINT_DISPLAY_NAME = f'{VERTEX_DATASET_NAME}-classifier'\n",
    "\n",
    "FEATURESTORE_ID = main_config['featurestore_id']\n",
    "\n",
    "CF_REGION = main_config['cloudfunction_region']\n",
    "\n",
    "DATAFLOW_SUBNETWORK = f\"https://www.googleapis.com/compute/v1/projects/{PROJECT}/regions/{REGION}/subnetworks/{main_config['dataflow']['subnet']}\"\n",
    "DATAFLOW_SERVICE_ACCOUNT = main_config['dataflow']['service_account']\n",
    "\n",
    "CLOUDBUILD_SA = f'projects/{PROJECT}/serviceAccounts/{SERVICE_ACCOUNT}'\n",
    "\n",
    "LIMIT=main_config['limit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda3ab18-7560-4d47-a911-101d21e8bfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: student-mlops5\n",
      "Region: us-central1\n",
      "Service Account: 743451655808-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68df038-006c-4667-ac74-0123c4789109",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb98a921-cba7-46ee-baa7-b0a4461c3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VERTEX_DATASET_NAME\"] = VERTEX_DATASET_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{VERTEX_DATASET_NAME}\"\n",
    "os.environ[\"MODEL_REGISTRY_URI\"] = os.path.join(os.environ[\"GCS_LOCATION\"], \"model_registry\")\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"8000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"8000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DataflowRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"vertex\"\n",
    "os.environ[\"DATAFLOW_IMAGE_URI\"] = f\"{DOCKER_REPO}/dataflow:latest\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"{DOCKER_REPO}/vertex:latest\"\n",
    "os.environ[\"ENABLE_CACHE\"] = \"1\"\n",
    "os.environ[\"SUBNETWORK\"] = DATAFLOW_SUBNETWORK\n",
    "os.environ[\"SERVICE_ACCOUNT\"] = DATAFLOW_SERVICE_ACCOUNT\n",
    "os.environ[\"BQ_LOCATION\"] = BQ_LOCATION\n",
    "os.environ[\"BQ_DATASET_NAME\"] = BQ_DATASET_NAME\n",
    "os.environ[\"ML_TABLE\"] = ML_TABLE\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{VERTEX_DATASET_NAME}/e2e_tests\"\n",
    "os.environ[\"SUBNETWORK\"] = DATAFLOW_SUBNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d64749c-b1c6-49d6-b8fa-3a18e5bddc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"UPLOAD_MODEL\"] = \"0\"\n",
    "os.environ[\"ACCURACY_THRESHOLD\"] = \"-0.1\"    # NB Negative accuracy threshold makes no sense - allows everything\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d3b8465-c78c-4989-b920-3acd536c3e1c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: student-mlops5\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://student-mlops5/creditcards/e2e_tests\n",
      "DOCKER_REPO_NAME: docker-repo\n",
      "ARTIFACT_STORE_URI: gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://student-mlops5/creditcards/model_registry\n",
      "VERTEX_DATASET_NAME: creditcards\n",
      "MODEL_DISPLAY_NAME: creditcards-classifier-v02\n",
      "PIPELINE_NAME: creditcards-classifier-v02-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 8000\n",
      "TEST_LIMIT: 8000\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: -0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: us-central1-docker.pkg.dev/student-mlops5/creditcards/vertex:latest\n",
      "DATAFLOW_IMAGE_URI: us-central1-docker.pkg.dev/student-mlops5/creditcards/dataflow:latest\n",
      "BEAM_RUNNER: DirectRunner\n",
      "SERVICE_ACCOUNT: 743451655808-compute@developer.gserviceaccount.com\n",
      "SUBNETWORK: https://www.googleapis.com/compute/v1/projects/student-mlops5/regions/us-central1/subnetworks/${subnetwork}\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=student-mlops5', '--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=student-mlops5', '--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp', '--region=us-central1', '--runner=DirectRunner', '--service_account_email=743451655808-compute@developer.gserviceaccount.com', '--no_use_public_ips', '--subnetwork=https://www.googleapis.com/compute/v1/projects/student-mlops5/regions/us-central1/subnetworks/${subnetwork}', '--sdk_container_image=us-central1-docker.pkg.dev/student-mlops5/creditcards/dataflow:latest']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'student-mlops5', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'us-central1-docker.pkg.dev/student-mlops5/creditcards/vertex:latest'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'us-central1', 'ai_platform_training_args': {'project': 'student-mlops5', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'us-central1-docker.pkg.dev/student-mlops5/creditcards/vertex:latest'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://student-mlops5/creditcards/e2e_tests/temp', 'gcs_location': 'gs://student-mlops5/creditcards/e2e_tests/temp', 'project': 'student-mlops5', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: creditcards-classifier-v02-predictions\n",
      "ENABLE_CACHE: 1\n",
      "UPLOAD_MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70c02a7f-8545-42aa-b55a-1b864ccefed8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.2.0\n",
      "rootdir: /home/jupyter/rafal\n",
      "plugins: anyio-3.7.1, typeguard-2.13.3\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/datasource_utils_tests.py BigQuery Source: student-mlops5.creditcards.creditcards_ml\n",
      "\u001b[32m.\u001b[0mBigQuery Source: student-mlops5.creditcards.creditcards_ml\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.7/site-packages/google/rpc/__init__.py:18\n",
      "  /opt/conda/lib/python3.7/site-packages/google/rpc/__init__.py:18: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: 19 warnings\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: 15 warnings\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2350\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2350\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2350\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(parent)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.iam')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/google/rpc/__init__.py:20\n",
      "  /opt/conda/lib/python3.7/site-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    pkg_resources.declare_namespace(__name__)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py:37\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py:37: DeprecationWarning: Support for grpcio-gcp is deprecated. This feature will be\n",
      "          removed from `google-api-core` after January 1, 2024. If you need to\n",
      "          continue to use this feature, please pin to a specific version of\n",
      "          `google-api-core`.\n",
      "    DeprecationWarning,\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m42 warnings\u001b[0m\u001b[33m in 5.06s\u001b[0m\u001b[33m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest src/tests/datasource_utils_tests.py -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5977100-f70d-4d75-bb8b-c88340d378a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.2.0\n",
      "rootdir: /home/jupyter/rafal\n",
      "plugins: anyio-3.7.1, typeguard-2.13.3\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/model_tests.py \u001b[32m.\u001b[0m2024-02-23 08:52:04.202454: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-02-23 08:52:04.202507: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-02-23 08:52:04.202551: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-20240206-073850): /proc/driver/nvidia/version does not exist\n",
      "2024-02-23 08:52:04.203009: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['V1[0][0]',                     \n",
      "                                                                  'V2[0][0]',                     \n",
      "                                                                  'V3[0][0]',                     \n",
      "                                                                  'V4[0][0]',                     \n",
      "                                                                  'V5[0][0]',                     \n",
      "                                                                  'V6[0][0]',                     \n",
      "                                                                  'V7[0][0]',                     \n",
      "                                                                  'V8[0][0]',                     \n",
      "                                                                  'V9[0][0]',                     \n",
      "                                                                  'V10[0][0]',                    \n",
      "                                                                  'V11[0][0]',                    \n",
      "                                                                  'V12[0][0]',                    \n",
      "                                                                  'V13[0][0]',                    \n",
      "                                                                  'V14[0][0]',                    \n",
      "                                                                  'V15[0][0]',                    \n",
      "                                                                  'V16[0][0]',                    \n",
      "                                                                  'V17[0][0]',                    \n",
      "                                                                  'V18[0][0]',                    \n",
      "                                                                  'V19[0][0]',                    \n",
      "                                                                  'V20[0][0]',                    \n",
      "                                                                  'V21[0][0]',                    \n",
      "                                                                  'V22[0][0]',                    \n",
      "                                                                  'V23[0][0]',                    \n",
      "                                                                  'V24[0][0]',                    \n",
      "                                                                  'V25[0][0]',                    \n",
      "                                                                  'V26[0][0]',                    \n",
      "                                                                  'V27[0][0]',                    \n",
      "                                                                  'V28[0][0]',                    \n",
      "                                                                  'Amount[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           1920        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,033\n",
      "Trainable params: 4,033\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 3.16s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest src/tests/model_tests.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e973b-ed67-4933-8b83-23e514bb244c",
   "metadata": {},
   "source": [
    "#### End to end pipeline unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ad9e768-4c25-4765-aeac-9e47f6c6cc70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.2.0\n",
      "rootdir: /home/jupyter/rafal\n",
      "plugins: anyio-3.7.1, typeguard-2.13.3\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py upload_model: 0\n",
      "Pipeline e2e test artifacts stored in: gs://student-mlops5/creditcards/e2e_tests\n",
      "ML metadata store is ready.\n",
      "Using dataset creditcards\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Labels for model: {\"dataset_name\": \"creditcards\", \"pipeline_name\": \"creditcards-classifier-v02-train-pipeline\", \"pipeline_root\": \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/credit\"}\n",
      "Pipeline components: ['HyperparamsGen', 'TrainDataGen', 'TestDataGen', 'StatisticsGen', 'SchemaImporter', 'ExampleValidator', 'DataTransformer', 'WarmstartModelResolver', 'ModelTrainer', 'BaselineModelResolver', 'ModelEvaluator', 'GcsModelPusher']\n",
      "Beam pipeline args: ['--project=student-mlops5', '--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp']\n",
      "Beam pipeline args: ['--project=student-mlops5', '--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp']\n",
      "Generating ephemeral wheel package for '/home/jupyter/rafal/src/preprocessing/transformations.py' (including modules: ['transformations']).\n",
      "User module package has hash fingerprint version 5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.\n",
      "Executing: ['/opt/conda/bin/python', '/var/tmp/tmp_xcy_2vy/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/var/tmp/tmptgxgapyy', '--dist-dir', '/var/tmp/tmp94012bdg']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying transformations.py -> build/lib\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "installing to /var/tmp/tmptgxgapyy\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformations.py -> /var/tmp/tmptgxgapyy\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /var/tmp/tmptgxgapyy/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmptgxgapyy/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/WHEEL\n",
      "creating '/var/tmp/tmp94012bdg/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl' and adding '/var/tmp/tmptgxgapyy' to it\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/RECORD'\n",
      "removing /var/tmp/tmptgxgapyy\n",
      "Successfully built user code wheel distribution at 'gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl'; target user module is 'transformations'.\n",
      "Full user module path is 'transformations@gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl'\n",
      "Generating ephemeral wheel package for '/home/jupyter/rafal/src/model_training/runner.py' (including modules: ['defaults', 'runner', 'model', 'data', 'exporter', 'trainer']).\n",
      "User module package has hash fingerprint version 64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.\n",
      "Executing: ['/opt/conda/bin/python', '/var/tmp/tmp3alf08po/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/var/tmp/tmp9a_c0jwe', '--dist-dir', '/var/tmp/tmpqgtm2ccg']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying runner.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying data.py -> build/lib\n",
      "copying exporter.py -> build/lib\n",
      "copying trainer.py -> build/lib\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "installing to /var/tmp/tmp9a_c0jwe\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/exporter.py -> /var/tmp/tmp9a_c0jwe\n",
      "copying build/lib/runner.py -> /var/tmp/tmp9a_c0jwe\n",
      "copying build/lib/data.py -> /var/tmp/tmp9a_c0jwe\n",
      "copying build/lib/model.py -> /var/tmp/tmp9a_c0jwe\n",
      "copying build/lib/defaults.py -> /var/tmp/tmp9a_c0jwe\n",
      "copying build/lib/trainer.py -> /var/tmp/tmp9a_c0jwe\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /var/tmp/tmp9a_c0jwe/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmp9a_c0jwe/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/WHEEL\n",
      "creating '/var/tmp/tmpqgtm2ccg/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7-py3-none-any.whl' and adding '/var/tmp/tmp9a_c0jwe' to it\n",
      "adding 'data.py'\n",
      "adding 'defaults.py'\n",
      "adding 'exporter.py'\n",
      "adding 'model.py'\n",
      "adding 'runner.py'\n",
      "adding 'trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/RECORD'\n",
      "removing /var/tmp/tmp9a_c0jwe\n",
      "Successfully built user code wheel distribution at 'gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7-py3-none-any.whl'; target user module is 'runner'.\n",
      "Full user module path is 'runner@gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7-py3-none-any.whl'\n",
      "Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"DataTransformer\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=student-mlops5\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"GcsModelPusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"HyperparamsGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.hyperparameters_gen_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelEvaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=student-mlops5\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelTrainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=student-mlops5\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=student-mlops5\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=student-mlops5\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"mlmd.sqllite\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"mlmd.sqllite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "Component BaselineModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"BaselineModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.BaselineModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"model\"\n",
      "      input_keys: \"model_blessing\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type ModelBlessing is not found in MLMD.\n",
      "Artifact type Model is not found in MLMD.\n",
      "Component BaselineModelResolver is finished.\n",
      "Component HyperparamsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 2\n",
      "Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={}, output_dict=defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:HyperparamsGen:hyperparameters:0\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}), exec_properties={'hidden_units': '128,128', 'learning_rate': 0.001, 'num_epochs': 1, 'batch_size': 512}, execution_output_uri='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/stateful_working_dir/2024-02-23T08:52:24.522512', tmp_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2024-02-23T08:52:24.522512')\n",
      "Hyperparameters: {'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "Hyperparameters are written to: gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2/hyperparameters.json\n",
      "Cleaning up stateless execution info.\n",
      "Execution 2 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:HyperparamsGen:hyperparameters:0\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}) for execution 2\n",
      "MetadataStore with DB connection initialized\n",
      "Component HyperparamsGen is finished.\n",
      "Component SchemaImporter is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "  }\n",
      "  id: \"SchemaImporter\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"result\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"artifact_uri\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"src/raw_schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"reimport\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an importer node.\n",
      "MetadataStore with DB connection initialized\n",
      "Processing source uri: src/raw_schema, properties: {}, custom_properties: {}\n",
      "Component SchemaImporter is finished.\n",
      "Component TestDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM creditcards.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 8000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 4\n",
      "Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TestDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM creditcards.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 8000\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'output_file_format': 5, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}', 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir/2024-02-23T08:52:24.522512', tmp_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM creditcards.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 8000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2024-02-23T08:52:24.522512')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /home/jupyter/.local/lib/python3.7/site-packages/tfx to temp dir /var/tmp/tmpyostdlzo/build/tfx\n",
      "Generating a temp setup file at /var/tmp/tmpyostdlzo/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /var/tmp/tmpyostdlzo/build/tfx/setup.log\n",
      "Added --extra_package=/var/tmp/tmpyostdlzo/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f4f9790eb90> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f4f9790ecb0> ====================\n",
      "==================== <function pack_combiners at 0x7f4f9790f200> ====================\n",
      "==================== <function lift_combiners at 0x7f4f9790f290> ====================\n",
      "==================== <function expand_sdf at 0x7f4f9790f440> ====================\n",
      "==================== <function expand_gbk at 0x7f4f9790f4d0> ====================\n",
      "==================== <function sink_flattens at 0x7f4f9790f5f0> ====================\n",
      "==================== <function greedily_fuse at 0x7f4f9790f680> ====================\n",
      "==================== <function read_to_impulse at 0x7f4f9790f710> ====================\n",
      "==================== <function impulse_to_input at 0x7f4f9790f7a0> ====================\n",
      "==================== <function sort_stages at 0x7f4f9790f9e0> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f4f9790fb00> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f4f9790f950> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f4f9790fa70> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f4f94521350> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Setting socket default timeout to 60 seconds.\n",
      "socket default timeout is 60.0 seconds.\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'us-central1'\n",
      " projectId: 'student-mlops5'>\n",
      " bq show -j --format=prettyjson --project_id=student-mlops5 None\n",
      "Using location 'us-central1' from table <TableReference\n",
      " datasetId: 'creditcards'\n",
      " projectId: 'student-mlops5'\n",
      " tableId: 'creditcards_ml'> referenced by query \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM creditcards.creditcards_ml \n",
      "    WHERE ML_use = 'TEST'\n",
      "    LIMIT 8000\n",
      "Dataset student-mlops5:beam_temp_dataset_1c70ce5b50c344e78b2edb1a16f1d3db does not exist so we will create it as temporary with location=us-central1\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_65a78110-8_1708678354_34'\n",
      " location: 'us-central1'\n",
      " projectId: 'student-mlops5'>\n",
      " bq show -j --format=prettyjson --project_id=student-mlops5 beam_bq_job_QUERY_BQ_EXPORT_JOB_65a78110-8_1708678354_34\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_65a78110-8_1708678360_335'\n",
      " location: 'us-central1'\n",
      " projectId: 'student-mlops5'>\n",
      " bq show -j --format=prettyjson --project_id=student-mlops5 beam_bq_job_EXPORT_BQ_EXPORT_JOB_65a78110-8_1708678360_335\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0567469596862793 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05141568183898926 seconds.\n",
      "Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.042298078536987305 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.054192543029785156 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.046730756759643555 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 4 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TestDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 4\n",
      "MetadataStore with DB connection initialized\n",
      "Component TestDataGen is finished.\n",
      "Component TrainDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM creditcards.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 8000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 5\n",
      "Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TrainDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM creditcards.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 8000\"\\n    }\\n  ]\\n}', 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir/2024-02-23T08:52:24.522512', tmp_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM creditcards.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 8000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2024-02-23T08:52:24.522512')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /home/jupyter/.local/lib/python3.7/site-packages/tfx to temp dir /var/tmp/tmpj0phteop/build/tfx\n",
      "Generating a temp setup file at /var/tmp/tmpj0phteop/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /var/tmp/tmpj0phteop/build/tfx/setup.log\n",
      "Added --extra_package=/var/tmp/tmpj0phteop/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f4f9790eb90> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f4f9790ecb0> ====================\n",
      "==================== <function pack_combiners at 0x7f4f9790f200> ====================\n",
      "==================== <function lift_combiners at 0x7f4f9790f290> ====================\n",
      "==================== <function expand_sdf at 0x7f4f9790f440> ====================\n",
      "==================== <function expand_gbk at 0x7f4f9790f4d0> ====================\n",
      "==================== <function sink_flattens at 0x7f4f9790f5f0> ====================\n",
      "==================== <function greedily_fuse at 0x7f4f9790f680> ====================\n",
      "==================== <function read_to_impulse at 0x7f4f9790f710> ====================\n",
      "==================== <function impulse_to_input at 0x7f4f9790f7a0> ====================\n",
      "==================== <function sort_stages at 0x7f4f9790f9e0> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f4f9790fb00> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f4f9790f950> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f4f9790fa70> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f4f94399310> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'us-central1'\n",
      " projectId: 'student-mlops5'>\n",
      " bq show -j --format=prettyjson --project_id=student-mlops5 None\n",
      "Using location 'us-central1' from table <TableReference\n",
      " datasetId: 'creditcards'\n",
      " projectId: 'student-mlops5'\n",
      " tableId: 'creditcards_ml'> referenced by query \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM creditcards.creditcards_ml \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 8000\n",
      "Dataset student-mlops5:beam_temp_dataset_2faa7d22b4d84a91823968b83a9c8c4d does not exist so we will create it as temporary with location=us-central1\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_f512cb7b-2_1708678379_795'\n",
      " location: 'us-central1'\n",
      " projectId: 'student-mlops5'>\n",
      " bq show -j --format=prettyjson --project_id=student-mlops5 beam_bq_job_QUERY_BQ_EXPORT_JOB_f512cb7b-2_1708678379_795\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_f512cb7b-2_1708678384_125'\n",
      " location: 'us-central1'\n",
      " projectId: 'student-mlops5'>\n",
      " bq show -j --format=prettyjson --project_id=student-mlops5 beam_bq_job_EXPORT_BQ_EXPORT_JOB_f512cb7b-2_1708678384_125\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04466700553894043 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05564546585083008 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05094146728515625 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04384326934814453 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.06185317039489746 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04643678665161133 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05072474479675293 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.048761606216430664 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 5 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TrainDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 5\n",
      "MetadataStore with DB connection initialized\n",
      "Component TrainDataGen is finished.\n",
      "Component WarmstartModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"WarmstartModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.WarmstartModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"latest_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"latest_model\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type Model is not found in MLMD.\n",
      "Component WarmstartModelResolver is finished.\n",
      "Component StatisticsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 7\n",
      "Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TrainDataGen:examples:0\"\n",
      "create_time_since_epoch: 1708678396536\n",
      "last_update_time_since_epoch: 1708678396536\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:StatisticsGen:statistics:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/executor_output.pb', stateful_working_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir/2024-02-23T08:52:24.522512', tmp_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2024-02-23T08:52:24.522512')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /home/jupyter/.local/lib/python3.7/site-packages/tfx to temp dir /var/tmp/tmp49rv9z8l/build/tfx\n",
      "Generating a temp setup file at /var/tmp/tmp49rv9z8l/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /var/tmp/tmp49rv9z8l/build/tfx/setup.log\n",
      "Added --extra_package=/var/tmp/tmp49rv9z8l/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Generating statistics for split train.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04902005195617676 seconds.\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split train written to gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-train.\n",
      "Generating statistics for split eval.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04634714126586914 seconds.\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split eval written to gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-eval.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f4f9790eb90> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f4f9790ecb0> ====================\n",
      "==================== <function pack_combiners at 0x7f4f9790f200> ====================\n",
      "==================== <function lift_combiners at 0x7f4f9790f290> ====================\n",
      "==================== <function expand_sdf at 0x7f4f9790f440> ====================\n",
      "==================== <function expand_gbk at 0x7f4f9790f4d0> ====================\n",
      "==================== <function sink_flattens at 0x7f4f9790f5f0> ====================\n",
      "==================== <function greedily_fuse at 0x7f4f9790f680> ====================\n",
      "==================== <function read_to_impulse at 0x7f4f9790f710> ====================\n",
      "==================== <function impulse_to_input at 0x7f4f9790f7a0> ====================\n",
      "==================== <function sort_stages at 0x7f4f9790f9e0> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f4f9790fb00> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f4f9790f950> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f4f9790fa70> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f4f93ed64d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.048926353454589844 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0495455265045166 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0511479377746582 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.06445860862731934 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04957103729248047 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04973745346069336 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 7 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:StatisticsGen:statistics:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 7\n",
      "MetadataStore with DB connection initialized\n",
      "Component StatisticsGen is finished.\n",
      "Component ExampleValidator is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 8\n",
      "Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'statistics': [Artifact(artifact: id: 5\n",
      "type_id: 22\n",
      "uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:StatisticsGen:statistics:0\"\n",
      "create_time_since_epoch: 1708678407244\n",
      "last_update_time_since_epoch: 1708678407244\n",
      ", artifact_type: id: 22\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1708678347734\n",
      "last_update_time_since_epoch: 1708678347734\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:ExampleValidator:anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/executor_output.pb', stateful_working_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir/2024-02-23T08:52:24.522512', tmp_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2024-02-23T08:52:24.522512')\n",
      "Validating schema against the computed statistics for split train.\n",
      "Validation complete for split train. Anomalies written to gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-train.\n",
      "Validating schema against the computed statistics for split eval.\n",
      "Validation complete for split eval. Anomalies written to gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-eval.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 8 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:ExampleValidator:anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}) for execution 8\n",
      "MetadataStore with DB connection initialized\n",
      "Component ExampleValidator is finished.\n",
      "Component DataTransformer is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 9\n",
      "Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1708678347734\n",
      "last_update_time_since_epoch: 1708678347734\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")], 'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:TrainDataGen:examples:0\"\n",
      "create_time_since_epoch: 1708678396536\n",
      "last_update_time_since_epoch: 1708678396536\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pre_transform_schema': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:pre_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:pre_transform_schema:0\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_anomalies/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:post_transform_anomalies:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:post_transform_anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'updated_analyzer_cache': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/updated_analyzer_cache/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:updated_analyzer_cache:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:updated_analyzer_cache:0\"\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:post_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:post_transform_schema:0\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:transformed_examples:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:transformed_examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'pre_transform_stats': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:pre_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:pre_transform_stats:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'transform_graph': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:transform_graph:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:transform_graph:0\"\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:post_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2024-02-23T08:52:24.522512:DataTransformer:post_transform_stats:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'force_tf_compat_v1': 0, 'disable_statistics': 0, 'module_path': 'transformations@gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl', 'custom_config': 'null', 'splits_config': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}'}, execution_output_uri='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/executor_output.pb', stateful_working_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir/2024-02-23T08:52:24.522512', tmp_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-02-23T08:52:24.522512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-02-23T08:52:24.522512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2024-02-23T08:52:24.522512')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /home/jupyter/.local/lib/python3.7/site-packages/tfx to temp dir /var/tmp/tmptbpd087c/build/tfx\n",
      "Generating a temp setup file at /var/tmp/tmptbpd087c/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /var/tmp/tmptbpd087c/build/tfx/setup.log\n",
      "Added --extra_package=/var/tmp/tmptbpd087c/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "Installing '/var/tmp/tmpkaz6ajzw/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/var/tmp/tmp4zkguym_', '/var/tmp/tmpkaz6ajzw/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl']\n",
      "Processing /var/tmp/tmpkaz6ajzw/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34\n",
      "Successfully installed '/var/tmp/tmpkaz6ajzw/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl'.\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "Installing '/var/tmp/tmphqx40l7y/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/var/tmp/tmpkib6e4hj', '/var/tmp/tmphqx40l7y/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl']\n",
      "Processing /var/tmp/tmphqx40l7y/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34\n",
      "Successfully installed '/var/tmp/tmphqx40l7y/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl'.\n",
      "Installing '/var/tmp/tmpbl7au7ok/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/var/tmp/tmpjh1nu38_', '/var/tmp/tmpbl7au7ok/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl']\n",
      "Processing /var/tmp/tmpbl7au7ok/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34\n",
      "Successfully installed '/var/tmp/tmpbl7au7ok/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl'.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "2024-02-23 08:53:52.099148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-02-23 08:53:52.099213: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-02-23 08:53:52.099240: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-20240206-073850): /proc/driver/nvidia/version does not exist\n",
      "2024-02-23 08:53:52.099607: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:326: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.056385040283203125 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04471993446350098 seconds.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.06036233901977539 seconds.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f4f9790eb90> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f4f9790ecb0> ====================\n",
      "==================== <function pack_combiners at 0x7f4f9790f200> ====================\n",
      "==================== <function lift_combiners at 0x7f4f9790f290> ====================\n",
      "==================== <function expand_sdf at 0x7f4f9790f440> ====================\n",
      "==================== <function expand_gbk at 0x7f4f9790f4d0> ====================\n",
      "==================== <function sink_flattens at 0x7f4f9790f5f0> ====================\n",
      "==================== <function greedily_fuse at 0x7f4f9790f680> ====================\n",
      "==================== <function read_to_impulse at 0x7f4f9790f710> ====================\n",
      "==================== <function impulse_to_input at 0x7f4f9790f7a0> ====================\n",
      "==================== <function sort_stages at 0x7f4f9790f9e0> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f4f9790fb00> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f4f9790f950> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f4f9790fa70> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f4f9428d990> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "2024-02-23 08:54:20.626525: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Assets written to: gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/633958d529624e7e8706de95a492108f/assets\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04605746269226074 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.045859575271606445 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04930591583251953 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05050158500671387 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04945540428161621 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05127549171447754 seconds.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.050075531005859375 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.21 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04995560646057129 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04811668395996094 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04293060302734375 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.042153358459472656 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04536318778991699 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04945635795593262 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.050533294677734375 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04050755500793457 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04893994331359863 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04494285583496094 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.06403803825378418 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0466921329498291 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04569101333618164 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05118870735168457 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0477147102355957 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.049753427505493164 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04436969757080078 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04351615905761719 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04829597473144531 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05689549446105957 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.048165082931518555 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04713606834411621 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05568861961364746 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05072450637817383 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05364561080932617 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0459597110748291 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.06799888610839844 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.049269914627075195 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04600167274475098 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04836702346801758 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04691743850708008 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04221057891845703 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.051259517669677734 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04671621322631836 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04666852951049805 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04988408088684082 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05573868751525879 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05560731887817383 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04570579528808594 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04734969139099121 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04663395881652832 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.046930789947509766 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04474449157714844 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04981684684753418 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04308485984802246 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.050482749938964844 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04959702491760254 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.049927473068237305 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04946112632751465 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04451251029968262 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.053887128829956055 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04326915740966797 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.048925161361694336 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04573822021484375 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0462038516998291 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04440426826477051 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.053658246994018555 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04933786392211914 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.051343441009521484 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04643416404724121 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04476046562194824 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04390764236450195 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04807925224304199 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04781675338745117 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04413866996765137 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04584145545959473 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04738450050354004 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04968714714050293 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.047905683517456055 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.16742777824401855 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05823636054992676 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05927085876464844 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.061147212982177734 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05099153518676758 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05670356750488281 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05409049987792969 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05144906044006348 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04704761505126953 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04381871223449707 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04410696029663086 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04945874214172363 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.044156789779663086 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04610300064086914 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04649233818054199 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.21 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04510974884033203 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04322981834411621 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04816889762878418 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.06012773513793945 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Assets written to: gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/5b8e6a014380444d9630a1cff22f3e26/assets\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04581451416015625 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04548907279968262 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04818153381347656 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.08194422721862793 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05574321746826172 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05601310729980469 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04514884948730469 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05238699913024902 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.059717416763305664 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Artifact type Model is not found in MLMD.\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Processing /var/tmp/tmp4z7gtowb/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-ModelTrainer\n",
      "Successfully installed tfx-user-code-ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7\n",
      "Runner started...\n",
      "fn_args: FnArgs(working_dir=None, train_files=['gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-train/*'], eval_files=['gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-eval/*'], train_steps=None, eval_steps=None, schema_path='src/raw_schema/schema.pbtxt', schema_file='src/raw_schema/schema.pbtxt', transform_graph_path='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', transform_output='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7f4f90109560>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7f4f90083d40>, data_view_decode_fn=None), serving_model_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving', eval_model_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-TFMA', model_run_dir='gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model_run/10', base_model=None, hyperparameters={'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}, custom_config=None)\n",
      "\n",
      "Hyperparameter:\n",
      "{'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "\n",
      "Runner executing trainer...\n",
      "Loading tft output from gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['Amount[0][0]',                 \n",
      "                                                                  'V1[0][0]',                     \n",
      "                                                                  'V10[0][0]',                    \n",
      "                                                                  'V11[0][0]',                    \n",
      "                                                                  'V12[0][0]',                    \n",
      "                                                                  'V13[0][0]',                    \n",
      "                                                                  'V14[0][0]',                    \n",
      "                                                                  'V15[0][0]',                    \n",
      "                                                                  'V16[0][0]',                    \n",
      "                                                                  'V17[0][0]',                    \n",
      "                                                                  'V18[0][0]',                    \n",
      "                                                                  'V19[0][0]',                    \n",
      "                                                                  'V2[0][0]',                     \n",
      "                                                                  'V20[0][0]',                    \n",
      "                                                                  'V21[0][0]',                    \n",
      "                                                                  'V22[0][0]',                    \n",
      "                                                                  'V23[0][0]',                    \n",
      "                                                                  'V24[0][0]',                    \n",
      "                                                                  'V25[0][0]',                    \n",
      "                                                                  'V26[0][0]',                    \n",
      "                                                                  'V27[0][0]',                    \n",
      "                                                                  'V28[0][0]',                    \n",
      "                                                                  'V3[0][0]',                     \n",
      "                                                                  'V4[0][0]',                     \n",
      "                                                                  'V5[0][0]',                     \n",
      "                                                                  'V6[0][0]',                     \n",
      "                                                                  'V7[0][0]',                     \n",
      "                                                                  'V8[0][0]',                     \n",
      "                                                                  'V9[0][0]']                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          3840        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            129         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,481\n",
      "Trainable params: 20,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model training started..\n",
      "24/24 [==============================] - 4s 118ms/step - loss: 0.1784 - accuracy: 0.9851 - auc: 0.0047 - val_loss: 0.0355 - val_accuracy: 0.9967 - val_auc: 0.0021\n",
      "Model training completed.\n",
      "Runner executing exporter...\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Model export started...\n",
      "Function `serve_features_fn` contains input name(s) Amount, V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26, V27, V28, V3, V4, V5, V6, V7, V8, V9 with unsupported characters which will be renamed to amount, v1, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v2, v20, v21, v22, v23, v24, v25, v26, v27, v28, v3, v4, v5, v6, v7, v8, v9 in the SavedModel.\n",
      "Assets written to: gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving/assets\n",
      "Model export completed.\n",
      "Runner completed.\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f4f2eb04e10> and <keras.engine.input_layer.InputLayer object at 0x7f4f946c0410>).\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.049501657485961914 seconds.\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f4f9402f190> and <keras.engine.input_layer.InputLayer object at 0x7f4f50a9ee90>).\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f4f9790eb90> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f4f9790ecb0> ====================\n",
      "==================== <function pack_combiners at 0x7f4f9790f200> ====================\n",
      "==================== <function lift_combiners at 0x7f4f9790f290> ====================\n",
      "==================== <function expand_sdf at 0x7f4f9790f440> ====================\n",
      "==================== <function expand_gbk at 0x7f4f9790f4d0> ====================\n",
      "==================== <function sink_flattens at 0x7f4f9790f5f0> ====================\n",
      "==================== <function greedily_fuse at 0x7f4f9790f680> ====================\n",
      "==================== <function read_to_impulse at 0x7f4f9790f710> ====================\n",
      "==================== <function impulse_to_input at 0x7f4f9790f7a0> ====================\n",
      "==================== <function sort_stages at 0x7f4f9790f9e0> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f4f9790fb00> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f4f9790f950> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f4f9790fa70> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f4f939fabd0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05294060707092285 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.05102229118347168 seconds.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f4f52ce9910> and <keras.engine.input_layer.InputLayer object at 0x7f4f52d85c50>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f4f2ead1c50> and <keras.engine.input_layer.InputLayer object at 0x7f4f905c1950>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f4f6c742e90> and <keras.engine.input_layer.InputLayer object at 0x7f4f901c6c10>).\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.051209211349487305 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f4f50df6d50> and <keras.engine.input_layer.InputLayer object at 0x7f4f5133f810>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f4f902c6f10> and <keras.engine.input_layer.InputLayer object at 0x7f4f2f13a050>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f4f2dee2b90> and <keras.engine.input_layer.InputLayer object at 0x7f4f2df6c0d0>).\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05780839920043945 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04651141166687012 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.06126856803894043 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04791665077209473 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04936838150024414 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.049506425857543945 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04803013801574707 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04285240173339844 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04610848426818848 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04797673225402832 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelEvaluator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "stateful_working_dir /home/jupyter/rafal/gs:/student-mlops5/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/GcsModelPusher/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Model output: gs://student-mlops5/creditcards/model_registry/creditcards-classifier-v02\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../.local/lib/python3.7/site-packages/apache_beam/runners/portability/stager.py:64\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/runners/portability/stager.py:64: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: 19 warnings\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: 15 warnings\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2350\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2350\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2350\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(parent)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.iam')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/google/rpc/__init__.py:20\n",
      "  /opt/conda/lib/python3.7/site-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    pkg_resources.declare_namespace(__name__)\n",
      "\n",
      "../.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py:37\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py:37: DeprecationWarning: Support for grpcio-gcp is deprecated. This feature will be\n",
      "          removed from `google-api-core` after January 1, 2024. If you need to\n",
      "          continue to use this feature, please pin to a specific version of\n",
      "          `google-api-core`.\n",
      "    DeprecationWarning,\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2471: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    temp_location = pcoll.pipeline.options.view_as(\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2473: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2504: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    | _PassThroughThenCleanup(files_to_remove_pcoll))\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m49 warnings\u001b[0m\u001b[33m in 254.69s (0:04:14)\u001b[0m\u001b[33m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest src/tests/pipeline_deployment_tests.py::test_e2e_pipeline -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cf4c2-a05d-41ce-af1a-cb7c97f0dfdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy to Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90774f24-3a81-4b6b-89ef-8dcd08387fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DirectRunner'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.BEAM_RUNNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a271d4e3-d45d-487f-972d-0b4e06db7758",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: student-mlops5\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://student-mlops5/creditcards/e2e_tests\n",
      "DOCKER_REPO_NAME: docker-repo\n",
      "ARTIFACT_STORE_URI: gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://student-mlops5/creditcards/model_registry\n",
      "VERTEX_DATASET_NAME: creditcards\n",
      "MODEL_DISPLAY_NAME: creditcards-classifier-v02\n",
      "PIPELINE_NAME: creditcards-classifier-v02-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 8000\n",
      "TEST_LIMIT: 8000\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: -0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: us-central1-docker.pkg.dev/student-mlops5/creditcards/vertex:latest\n",
      "DATAFLOW_IMAGE_URI: us-central1-docker.pkg.dev/student-mlops5/creditcards/dataflow:latest\n",
      "BEAM_RUNNER: DirectRunner\n",
      "SERVICE_ACCOUNT: 743451655808-compute@developer.gserviceaccount.com\n",
      "SUBNETWORK: https://www.googleapis.com/compute/v1/projects/student-mlops5/regions/us-central1/subnetworks/${subnetwork}\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=student-mlops5', '--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=student-mlops5', '--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp', '--region=us-central1', '--runner=DirectRunner', '--service_account_email=743451655808-compute@developer.gserviceaccount.com', '--no_use_public_ips', '--subnetwork=https://www.googleapis.com/compute/v1/projects/student-mlops5/regions/us-central1/subnetworks/${subnetwork}', '--sdk_container_image=us-central1-docker.pkg.dev/student-mlops5/creditcards/dataflow:latest']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'student-mlops5', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'us-central1-docker.pkg.dev/student-mlops5/creditcards/vertex:latest'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'us-central1', 'ai_platform_training_args': {'project': 'student-mlops5', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'us-central1-docker.pkg.dev/student-mlops5/creditcards/vertex:latest'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://student-mlops5/creditcards/e2e_tests/temp', 'gcs_location': 'gs://student-mlops5/creditcards/e2e_tests/temp', 'project': 'student-mlops5', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: creditcards-classifier-v02-predictions\n",
      "ENABLE_CACHE: 1\n",
      "UPLOAD_MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510db04-eb3b-4567-a81c-9a349a58e89e",
   "metadata": {},
   "source": [
    "### Create Repo for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d086a8-cd73-403f-92ed-6cd8d68321d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [creditcards]\n",
      "Waiting for operation [projects/student-mlops5/locations/us-central1/operations\n",
      "/64030d34-de2d-4809-96fa-e6fd9714b7fc] to complete...done.                     \n",
      "Created repository [creditcards].\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repo should has been created in the Terraform automation stage\n",
    "! gcloud artifacts repositories create {VERTEX_DATASET_NAME} --location={REGION} --repository-format=docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37f0cc-42ff-486c-92fd-ad2628348722",
   "metadata": {},
   "source": [
    "### Build Dataflow Worker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05123e22-d8f8-4cfd-a743-fe8c6e4925d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use build/Dockerfile.dataflow in case Internet access is not allowed\n",
    "!cp build/Dockerfile.dataflow Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5e9bf09-7555-4bea-b405-a1f657d15137",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DOCKER_REPO\"] = f\"{DOCKER_REPO}/vertex:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "715a8184-f534-4267-be00-b40fe6a7c14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 65 file(s) totalling 891.0 KiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2024.02.23/09.14.04.808060.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://student-mlops5_cloudbuild/source/1708679645.095858-90c9fc32fda44a41afcd2c69da4008e9.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/student-mlops5/locations/us-central1/builds/e0e9b972-1427-4834-803c-8be0b629e87e].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/e0e9b972-1427-4834-803c-8be0b629e87e?project=743451655808 ].\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                                    STATUS\n",
      "e0e9b972-1427-4834-803c-8be0b629e87e  2024-02-23T09:14:06+00:00  5M26S     gs://student-mlops5_cloudbuild/source/1708679645.095858-90c9fc32fda44a41afcd2c69da4008e9.tgz  us-central1-docker.pkg.dev/student-mlops5/creditcards/dataflow (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --project=$PROJECT --billing-project=$PROJECT --region $REGION --tag $DOCKER_REPO/dataflow:latest . --timeout=15m --machine-type=e2-highcpu-8 --suppress-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97d9ea-5a42-42a1-ba4c-7421e2acc052",
   "metadata": {},
   "source": [
    "### Build Vertex worker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2595e5e8-47bc-4f6d-8a39-b09d92588be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1-docker.pkg.dev/student-mlops5/creditcards/vertex:latest\n"
     ]
    }
   ],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12aff000-b7b8-4f1a-a2b7-2d4f56343a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90a2d12f-3c35-4dd7-993d-85e67b837876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp build/Dockerfile.vertex Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77943b76-0139-4d56-ba69-1c3485035500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 67 file(s) totalling 893.2 KiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2024.02.23/09.28.26.182880.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://student-mlops5_cloudbuild/source/1708680506.363734-bd93d9afec1e46fcaae0dfdfee9049a6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/student-mlops5/locations/us-central1/builds/0dfa5f9d-266e-4f35-a2aa-6bbad645b274].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/0dfa5f9d-266e-4f35-a2aa-6bbad645b274?project=743451655808 ].\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                                  STATUS\n",
      "0dfa5f9d-266e-4f35-a2aa-6bbad645b274  2024-02-23T09:28:26+00:00  8M12S     gs://student-mlops5_cloudbuild/source/1708680506.363734-bd93d9afec1e46fcaae0dfdfee9049a6.tgz  us-central1-docker.pkg.dev/student-mlops5/creditcards/vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --project=$PROJECT --billing-project=$PROJECT --region $REGION --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8 --suppress-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23bbe44-a6c8-4efa-b67f-d8c0ad3a1427",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27f79546-11d5-41f6-8307-bfe61e12fb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.tfx_pipelines' (namespace)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, sys\n",
    "importlib.reload(sys.modules['src.tfx_pipelines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e876c9eb-6f73-4ea9-a85c-0172c728c1d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for model: {\"dataset_name\": \"creditcards\", \"pipeline_name\": \"creditcards-classifier-v02-train-pipeline\", \"pipeline_root\": \"gs://student-mlops5/creditcards/e2e_tests/tfx_artifacts/credit\"}\n",
      "Beam pipeline args: ['--project=student-mlops5', '--temp_location=gs://student-mlops5/creditcards/e2e_tests/temp']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying transformations.py -> build/lib\n",
      "installing to /var/tmp/tmp6hicryi8\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformations.py -> /var/tmp/tmp6hicryi8\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /var/tmp/tmp6hicryi8/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmp6hicryi8/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/WHEEL\n",
      "creating '/var/tmp/tmp1vy1rnjp/tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34-py3-none-any.whl' and adding '/var/tmp/tmp6hicryi8' to it\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+5f756e0c080f4b923c81ace68c0608422bf252b532f40d34d1e2568e66fe2d34.dist-info/RECORD'\n",
      "removing /var/tmp/tmp6hicryi8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying runner.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying data.py -> build/lib\n",
      "copying exporter.py -> build/lib\n",
      "copying trainer.py -> build/lib\n",
      "installing to /var/tmp/tmp4wxxi92x\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/exporter.py -> /var/tmp/tmp4wxxi92x\n",
      "copying build/lib/runner.py -> /var/tmp/tmp4wxxi92x\n",
      "copying build/lib/data.py -> /var/tmp/tmp4wxxi92x\n",
      "copying build/lib/model.py -> /var/tmp/tmp4wxxi92x\n",
      "copying build/lib/defaults.py -> /var/tmp/tmp4wxxi92x\n",
      "copying build/lib/trainer.py -> /var/tmp/tmp4wxxi92x\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /var/tmp/tmp4wxxi92x/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmp4wxxi92x/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/WHEEL\n",
      "creating '/var/tmp/tmpjp271lqb/tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7-py3-none-any.whl' and adding '/var/tmp/tmp4wxxi92x' to it\n",
      "adding 'data.py'\n",
      "adding 'defaults.py'\n",
      "adding 'exporter.py'\n",
      "adding 'model.py'\n",
      "adding 'runner.py'\n",
      "adding 'trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+64ab11e4901ac5a1df1642484e956cf2ba9aa95553fb105341f41312bb9f36c7.dist-info/RECORD'\n",
      "removing /var/tmp/tmp4wxxi92x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config, runner\n",
    "\n",
    "pipeline_definition_file = f'{config.PIPELINE_NAME}.json'\n",
    "pipeline_definition = runner.compile_training_pipeline(pipeline_definition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e73afd4-c486-48b2-ba98-fe5d5bd56ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://creditcards-classifier-v02-train-pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 24.6 KiB/ 24.6 KiB]                                                \n",
      "Operation completed over 1 objects/24.6 KiB.                                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://student-mlops5/creditcards/compiled_pipelines/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINES_STORE = f\"gs://{BUCKET}/{VERTEX_DATASET_NAME}/compiled_pipelines/\"\n",
    "!gsutil cp {pipeline_definition_file} {PIPELINES_STORE}\n",
    "PIPELINES_STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df41e2b-e420-40c7-a08f-3b02a9667522",
   "metadata": {},
   "source": [
    "### Submit Vertex AI Pipelines run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26bd9a10-16cf-41de-ab55-65a301085dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creditcards-classifier-v02-train-pipeline.json'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_definition_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8d359e7-1679-49bd-8f1d-de1760c65cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'743451655808-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8516b764-1143-4b9e-9ae7-af9340146ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/creditcards-classifier-v02-train-pipeline-20240223100802?project=743451655808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/creditcards-classifier-v02-train-pipeline-20240223100802?project=743451655808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "    \n",
    "job = pipeline_jobs.PipelineJob(template_path = pipeline_definition_file,\n",
    "                                display_name=VERTEX_DATASET_NAME,\n",
    "                                enable_caching=False,\n",
    "                                parameter_values={\n",
    "                                    'learning_rate': 0.003,\n",
    "                                    'batch_size': 512,\n",
    "                                    'hidden_units': '128,128',\n",
    "                                    'num_epochs': 15,\n",
    "                                })\n",
    "\n",
    "job.run(sync=False, service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826bd3e-7bcc-4240-90b9-9b65ba36cc4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Continuous Training Pipeline (\"CI/CD\")\n",
    "\n",
    "\n",
    "### Build CI/CD image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e504cc07-22ee-45ba-9416-73cd22251c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1-docker.pkg.dev/student-mlops5/creditcards/cicd:latest'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CICD_IMAGE_URI = f\"{DOCKER_REPO}/cicd:latest\"\n",
    "CICD_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e3ffc-fd4e-4934-b2b2-995070ae2076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 13 file(s) totalling 19.2 KiB before compression.\n",
      "Uploading tarball of [build/.] to [gs://student-mlops5_cloudbuild/source/1708683606.770518-3aa3ac2e5cfa4204adb23b3aa7cd5e7b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/student-mlops5/locations/us-central1/builds/45e85e98-b378-4a6d-b39e-b7583933f60b].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/45e85e98-b378-4a6d-b39e-b7583933f60b?project=743451655808 ].\n",
      "PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/743451655808/locations/us-central1/pipelineJobs/creditcards-classifier-v02-train-pipeline-20240223100802 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "!cp build/Dockerfile.cicd-tfx build/Dockerfile\n",
    "!gcloud builds submit --project=$PROJECT --billing-project=$PROJECT --region $REGION --tag $CICD_IMAGE_URI build/. --timeout=15m --machine-type=e2-highcpu-8 --suppress-logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9512b-323e-474c-83d4-b9f595851934",
   "metadata": {},
   "source": [
    "### Automate the deployment of the Training Pipeline using Cloud Build\n",
    "***Important*** you should commit the code to the git repo since the next build process will checkout the code from the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7475bd2-90bd-4ad4-a7e8-f5ba1d72f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11b912-0069-4f05-afbd-b0738c9a33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['project_id'] = PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c1be1-2fd0-4da9-a50f-0885e01073a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f9daf-7602-48d7-92b4-0f265ba9bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = main_config['git']['repo_url']\n",
    "BRANCH = main_config['git']['branch']\n",
    "\n",
    "\n",
    "GCS_LOCATION = f\"gs://{BUCKET}/{VERTEX_DATASET_NAME}/\"\n",
    "TEST_GCS_LOCATION = f\"gs://{BUCKET}/{VERTEX_DATASET_NAME}/e2e_tests\"\n",
    "CI_TRAIN_LIMIT = 1000\n",
    "CI_TEST_LIMIT = 100\n",
    "CI_UPLOAD_MODEL = 0\n",
    "CI_ACCURACY_THRESHOLD = -0.1 # again setting accuracy threshold to negative\n",
    "BEAM_RUNNER = \"DataflowRunner\"\n",
    "TRAINING_RUNNER = \"vertex\"\n",
    "VERSION = 'latest'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "PIPELINES_STORE = f\"{GCS_LOCATION}compiled_pipelines/\"\n",
    "\n",
    "TFX_IMAGE_URI = f\"{DOCKER_REPO}/vertex:{VERSION}\"\n",
    "DATAFLOW_IMAGE_URI = f\"{DOCKER_REPO}/dataflow:latest\"\n",
    "\n",
    "REPO_NAME = REPO_URL.split('/')[-1][:-4]\n",
    "DESCR=f'\"Deploy train pipeline to GCS from {BRANCH}\"'\n",
    "\n",
    "\n",
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={REGION},\\\n",
    "_GCS_LOCATION={GCS_LOCATION},\\\n",
    "_TEST_GCS_LOCATION={TEST_GCS_LOCATION},\\\n",
    "_BQ_LOCATION={BQ_LOCATION},\\\n",
    "_BQ_DATASET_NAME={BQ_DATASET_NAME},\\\n",
    "_ML_TABLE={ML_TABLE},\\\n",
    "_VERTEX_DATASET_NAME={VERTEX_DATASET_NAME},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_CI_TRAIN_LIMIT={CI_TRAIN_LIMIT},\\\n",
    "_CI_TEST_LIMIT={CI_TEST_LIMIT},\\\n",
    "_CI_UPLOAD_MODEL={CI_UPLOAD_MODEL},\\\n",
    "_CI_ACCURACY_THRESHOLD={CI_ACCURACY_THRESHOLD},\\\n",
    "_BEAM_RUNNER={BEAM_RUNNER},\\\n",
    "_TRAINING_RUNNER={TRAINING_RUNNER},\\\n",
    "_DATAFLOW_IMAGE_URI={DATAFLOW_IMAGE_URI},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_PIPELINES_STORE={PIPELINES_STORE},\\\n",
    "_SUBNETWORK={DATAFLOW_SUBNETWORK},\\\n",
    "_GCS_BUCKET={BUCKET}/cloudbuild,\\\n",
    "_SERVICE_ACCOUNT={DATAFLOW_SERVICE_ACCOUNT},\\\n",
    "_WORKDIR={REPO_NAME}\\\n",
    "\"\"\"\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856ce61-fe1a-4db4-9c71-fed47129ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit build/known_hosts.github.zip --config build/pipeline-deployment-tfx.yaml --substitutions {SUBSTITUTIONS} --project=$PROJECT --billing-project=$PROJECT --region $REGION --suppress-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e622d-0708-404a-b4d5-63972ea6b441",
   "metadata": {},
   "source": [
    "### (Optional for Cloud Sources Repositories) Define the trigger that will deploy the pipeline after a commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928797f7-20cf-4769-9a35-24ad67900125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo gcloud beta builds triggers create cloud-source-repositories --repo={REPO_NAME} --branch-pattern=^{BRANCH}$ --description={DESCR} --build-config=mlops-creditcard/build/pipeline-deployment-tfx.yaml --substitutions={SUBSTITUTIONS} --billing-project={PROJECT}  --service-account={TRIGGER_SA}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99667732-3f1b-4c95-86e1-8001dff672dd",
   "metadata": {},
   "source": [
    "### Set up the trigger for the Training Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f56b2a-e55f-4b54-b90a-6335fce69314",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBSUB_TOPIC = f'trigger-{PIPELINE_NAME}'\n",
    "CLOUD_FUNCTION_NAME = f'trigger-{PIPELINE_NAME}-fn'\n",
    "GCS_PIPELINE_FILE_LOCATION = os.path.join(PIPELINES_STORE, f'{PIPELINE_NAME}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e210a-cca3-41e5-9083-ff3a395b9c1a",
   "metadata": {},
   "source": [
    "#### Create Pub/Sub Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ef492-ee1c-451d-a1e2-c5c511e00326",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create {PUBSUB_TOPIC}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547456c0-c55f-492e-97e6-f529182d3a13",
   "metadata": {},
   "source": [
    "#### Deploy Cloud Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff6cd1-f017-4230-9d32-9295f3c9cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d094091-3332-4c4c-9bb6-f6e5839d7ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_VARS=f\"\"\"\\\n",
    "PROJECT={PROJECT},\\\n",
    "REGION={REGION},\\\n",
    "GCS_PIPELINE_FILE_LOCATION={GCS_PIPELINE_FILE_LOCATION},\\\n",
    "SERVICE_ACCOUNT={SERVICE_ACCOUNT},\\\n",
    "PIPELINE_NAME={PIPELINE_NAME}\n",
    "\"\"\"\n",
    "\n",
    "!echo {ENV_VARS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da074b8-106c-42bf-b956-e59b249bf5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf src/pipeline_triggering/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a695eef-bf18-4fce-b44c-2ac00ae46675",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud functions deploy {CLOUD_FUNCTION_NAME} --gen2 \\\n",
    "    --region={CF_REGION} \\\n",
    "    --trigger-topic={PUBSUB_TOPIC} \\\n",
    "    --runtime=python38 \\\n",
    "    --source=src/pipeline_triggering\\\n",
    "    --entry-point=trigger_pipeline\\\n",
    "    --stage-bucket={BUCKET}\\\n",
    "    --ingress-settings=internal-only\\\n",
    "    --service-account={SERVICE_ACCOUNT}\\\n",
    "    --update-env-vars={ENV_VARS} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad5728-1b82-4979-bf16-fc518f8a9ba9",
   "metadata": {},
   "source": [
    "#### Test triggering the pipeline with a Pub/Sub message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead71b7d-fc63-4a80-8f52-e40f3612d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub\n",
    "import json\n",
    "\n",
    "publish_client = pubsub.PublisherClient()\n",
    "topic = f'projects/{PROJECT}/topics/{PUBSUB_TOPIC}'\n",
    "data = {\n",
    "    'num_epochs': 7,\n",
    "    'learning_rate': 0.0015,\n",
    "    'batch_size': 512,\n",
    "    'hidden_units': '256,126'\n",
    "}\n",
    "message = json.dumps(data)\n",
    "\n",
    "_ = publish_client.publish(topic, message.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e53359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c38ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d67b248-7ccb-4159-9ef2-2238455737f7",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff68dd-b3c4-4633-88fb-5997636d15ec",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "#### Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1793acf-21df-495f-b37b-99a54ec38cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_DISPLAY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088589e-d24a-47a8-9fe4-7a4135a8a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from build.utils import create_endpoint\n",
    "\n",
    "endpoint = create_endpoint(PROJECT, REGION, ENDPOINT_DISPLAY_NAME)\n",
    "endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603eb006-ae75-48ca-96d5-5cc22a786bc6",
   "metadata": {},
   "source": [
    "### Model Deployment Pipeline\n",
    "\n",
    "#### Run the model artifact testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394b2a9-aecd-4ec4-b0b7-f7ab8d2a5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"SERVICE_ACCOUNT\"] = SERVICE_ACCOUNT\n",
    "os.environ['ENDPOINT_DISPLAY_NAME'] = ENDPOINT_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e0efb-5ae3-4bda-b1cd-d6cc08f9b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest src/tests/model_deployment_tests.py::test_model_artifact -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a1d30-8dad-4f0a-854b-4de67c548d54",
   "metadata": {},
   "source": [
    "#### Deploy Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f0915-7257-41e0-bf33-2511671a439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python build/utils.py \\\n",
    "    --mode=deploy-model\\\n",
    "    --project={PROJECT}\\\n",
    "    --region={REGION}\\\n",
    "    --endpoint-display-name={ENDPOINT_DISPLAY_NAME}\\\n",
    "    --model-display-name={MODEL_DISPLAY_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402a585-5d45-4d9d-9497-cef77effb8a3",
   "metadata": {},
   "source": [
    "#### Test model on Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b7c0f-9a18-44cc-800e-a203903df6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest src/tests/model_deployment_tests.py::test_model_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a23c981-a4d2-4396-aa1f-3785244a81ca",
   "metadata": {},
   "source": [
    "#### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0a985-2955-4c33-9cbc-09fa630a1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = main_config['git']['repo_url']\n",
    "BRANCH = main_config['git']['branch']\n",
    "REPO_NAME = REPO_URL.split('/')[-1]\n",
    "CICD_IMAGE_URI = f\"{DOCKER_REPO}/cicd:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453e104-774d-475d-8159-328ea6ae0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={REGION},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_ENDPOINT_DISPLAY_NAME={ENDPOINT_DISPLAY_NAME},\\\n",
    "_GCS_BUCKET={BUCKET}/cloudbuild,\\\n",
    "_SERVICE_ACCOUNT={SERVICE_ACCOUNT},\\\n",
    "_WORKDIR={REPO_NAME}\\\n",
    "\"\"\"\n",
    "\n",
    "SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd627e0-aec5-4ff0-8bb3-b072aa01fc6d",
   "metadata": {},
   "source": [
    "### Test the build and define a manual trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8712b3-b431-4c82-b49f-2df4882c25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo gcloud builds submit --no-source --config build/model-deployment.yaml --substitutions {SUBSTITUTIONS} --billing-project {PROJECT} --suppress-logs --async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f6fae-4065-4d4e-bcb1-16585125f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCR=f'\"Deploy model from branch {BRANCH}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34927767-4aea-43b8-a0cb-df433dc591cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo gcloud alpha builds triggers create manual --repo={REPO_URL} --repo-type=CLOUD_SOURCE_REPOSITORIES --branch={BRANCH} --description={DESCR} --build-config=mlops-creditcard/build/model-deployment.yaml --substitutions={SUBSTITUTIONS} --billing-project={PROJECT} --service-account={CLOUDBUILD_SA}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d211c2e-8b5f-4511-845f-a96cefc7b274",
   "metadata": {},
   "source": [
    "## Deploy Prediction Cloud Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e70e8-b29a-4fc0-982f-8c6f1121d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = vertex_ai.Endpoint.list(\n",
    "    filter=f'display_name={ENDPOINT_DISPLAY_NAME}',\n",
    "    order_by=\"update_time\"\n",
    ")\n",
    "\n",
    "if len(endpoints) == 0:\n",
    "    print(f'No endpoints found with name {ENDPOINT_DISPLAY_NAME}')\n",
    "endpoint = endpoints[-1]\n",
    "\n",
    "os.environ['ENDPOINT_NAME'] = endpoint.name\n",
    "\n",
    "entity = 'user'\n",
    "os.environ['ENTITY'] = entity\n",
    "os.environ['FEATURESTORE_ID'] = FEATURESTORE_ID\n",
    "\n",
    "PREDICT_CLOUD_FUNCTION_NAME = \"predict-\" + PIPELINE_NAME + \"-fn\"\n",
    "PREDICT_CLOUD_FUNCTION_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa0135-99f3-45f9-b79d-0f04e34c4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tests.model_deployment_tests import test_instance\n",
    "test_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72229678-bea2-44a2-b79d-74124882b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [test_instance]\n",
    "predictions = endpoint.predict(test_instances).predictions\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613276f-a40a-4c4e-b250-c1c488c3331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "request = {'instances': test_instances}\n",
    "\n",
    "REQ_JSON=json.dumps(request)\n",
    "REQ_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b77ee8-31ca-4c19-8ab7-7a6a1f02e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJECT_ID'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['ENDPOINT_ID'] = endpoint.name\n",
    "os.environ['INPUT_DATA_FILE'] = \"INPUT-JSON\"\n",
    "os.environ['REQ_JSON'] = REQ_JSON\n",
    "!echo ${REQ_JSON} > ${INPUT_DATA_FILE}\n",
    "!curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\"  -H \"Content-Type: application/json\"  https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/europe-west4/endpoints/${ENDPOINT_ID}:predict  -d \"@${INPUT_DATA_FILE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5070311-5ad6-4afd-9baa-8420aaacd149",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Store (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f89f3a-9456-4f5c-af84-256fd51e3cb6",
   "metadata": {},
   "source": [
    "### Create Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff735bf-43a6-4fe1-b3c1-055b21860aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform_v1beta1 import FeaturestoreOnlineServingServiceClient, FeaturestoreServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76b8fe-3b31-4722-b95a-3f9330e91530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_store import feature_store as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2103aa-5845-4f51-a982-942b491b3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.create_fs(PROJECT, REGION, FEATURESTORE_ID, \"Feature Store for credit card use case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531af74-593a-4439-9c5a-5a45d320d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import operations_v1\n",
    "from google.cloud.aiplatform_v1beta1 import FeaturestoreOnlineServingServiceClient, FeaturestoreServiceClient, FeatureSelector\n",
    "from google.cloud.aiplatform_v1beta1.types import featurestore_online_service as featurestore_online_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import entity_type as entity_type_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import featurestore as featurestore_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import featurestore_service as featurestore_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import ListFeaturestoresRequest, CreateFeaturestoreRequest, Featurestore, ListEntityTypesRequest\n",
    "\n",
    "from google.protobuf.timestamp_pb2 import Timestamp\n",
    "from google.cloud.aiplatform_v1beta1.types import featurestore_monitoring as featurestore_monitoring_pb2\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"  \n",
    "admin_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": API_ENDPOINT})\n",
    "parent = f'{admin_client.common_location_path(PROJECT, REGION)}/featurestores/{FEATURESTORE_ID}'\n",
    "request = ListEntityTypesRequest(parent=parent)\n",
    "\n",
    "# Make the request\n",
    "page_result = admin_client.list_entity_types(request=request)\n",
    "\n",
    "# Handle the response\n",
    "[x.name.split('/')[-1] for x in page_result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e1109-23c9-4df1-9dcd-49579abd4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client.featurestore_path(PROJECT, REGION, FEATURESTORE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b2803-8136-4aae-9d32-370daf566826",
   "metadata": {},
   "source": [
    "#### Create an entity with features, generate some data and upload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e3a9f-1c49-452b-b41d-54b9c766fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = 'user'\n",
    "entity_descr = 'User ID'\n",
    "features = ['v27', 'v28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a0401-bf30-4433-b6ce-3ce7908bfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.create_entity(PROJECT, REGION, FEATURESTORE_ID, entity, entity_descr, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b258871-b60a-4841-9d87-974e6b7591d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "filename = f'features_{entity}.csv'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    line = f'{entity},{\",\".join(features)}\\n'\n",
    "    f.write(line)\n",
    "    for i in range(100):\n",
    "        f.write(f'user{i},{random.random()},{random.random()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643c891-971f-4fcc-aa32-608881e3ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -20 {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245990d-362c-48bd-a006-a39ba04225f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3660a3c-a615-4c09-b9a9-329bc56f7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp {filename} gs://{BUCKET}/{filename} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534dbb9a-f3d8-41f4-a6a7-d78dcc615ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_uris = [f'gs://{BUCKET}/{filename}']\n",
    "\n",
    "fs.ingest_entities_csv(PROJECT, REGION, FEATURESTORE_ID, entity, features, gcs_uris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fd50e-7a4c-475d-9834-eafad695bacb",
   "metadata": {},
   "source": [
    "Test reading some features back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ff3e3-cb67-476c-9e4b-a85fcfe1cf42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_data = {}\n",
    "for i in range(90,102):\n",
    "    entity_id = f'user{i}'\n",
    "    features_data[entity_id] = fs.read_features(PROJECT, REGION, FEATURESTORE_ID, entity, features, entity_id)\n",
    "\n",
    "features_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a116aeb-0366-4876-91dc-5ccd4a81a402",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy Prediction Cloud Function to use with Feature Store\n",
    "\n",
    "\n",
    "#### Test the enpoint with Feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101175fb-4cfc-42bf-8b47-87d9a19429ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tests.model_deployment_tests import test_instance\n",
    "\n",
    "import base64\n",
    "\n",
    "if 'V27' in test_instance:\n",
    "    del test_instance['V27']\n",
    "if 'V28' in test_instance:\n",
    "    del test_instance['V28']\n",
    "test_instance['userid'] = 'user99'\n",
    "\n",
    "test_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbd8f2-46a2-4e58-80f0-c4f7f29b8bb7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from src.prediction_cf.main import predict\n",
    "\n",
    "app = Flask('test')\n",
    "ctx = app.test_request_context(json=test_instance)\n",
    "request = ctx.request\n",
    "\n",
    "pred_retval = predict(request)\n",
    "pred_retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a69bc0-dbd0-4278-9095-52a639202ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_FUNCTION_SOURCE ='src/prediction_cf/main.py'\n",
    "\n",
    "ENV_VARS=f\"\"\"\\\n",
    "PROJECT={PROJECT},\\\n",
    "REGION={REGION},\\\n",
    "ENDPOINT_NAME={endpoint.name},\\\n",
    "ENTITY={entity},\\\n",
    "FEATURESTORE_ID={FEATURESTORE_ID}\n",
    "\"\"\"\n",
    "\n",
    "!echo {ENV_VARS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db24101-466a-4a90-b8d9-fbf3539ab33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf src/prediction_cf/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1106d-ee2f-4e49-a4f0-ab89d6c1f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo gcloud functions deploy {PREDICT_CLOUD_FUNCTION_NAME} --gen2 \\\n",
    "    --set-build-env-vars=GOOGLE_FUNCTION_SOURCE={GOOGLE_FUNCTION_SOURCE} \\\n",
    "    --region={CF_REGION} \\\n",
    "    --runtime=python38 \\\n",
    "    --trigger-http \\\n",
    "    --source=. \\\n",
    "    --entry-point=predict\\\n",
    "    --stage-bucket={BUCKET}\\\n",
    "    --ingress-settings=internal-only\\\n",
    "    --service-account={SERVICE_ACCOUNT}\\\n",
    "    --set-env-vars={ENV_VARS}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c77227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m112"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
